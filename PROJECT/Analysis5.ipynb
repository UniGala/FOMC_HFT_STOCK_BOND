{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 5 : Price/Volumes ~ FOMC Announcement #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter, MultipleLocator\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from scipy.stats import ttest_ind\n",
    "from pathlib import Path\n",
    "DATA_ROOT = Path('../data')\n",
    "RAW_ROOT = DATA_ROOT / 'raw'\n",
    "OUTPUT_ROOT = Path('../output')\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the FOMC narrative data and high-frequency trading data\n",
    "fomc_data = pd.read_csv(RAW_ROOT / 'FOMC_FFR.csv')\n",
    "hft_data = pd.read_csv(RAW_ROOT / 'HFT_IRF.csv')\n",
    "\n",
    "# Convert date/time columns to datetime format\n",
    "# ?? CSV ? DATE ??? \"1991/2/7\"?TIME ??? \"11:30:00\"\n",
    "fomc_data['DateTime'] = pd.to_datetime(fomc_data['DATE'] + ' ' + fomc_data['TIME'], format='%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "# ? DateTime ??????\n",
    "cols = fomc_data.columns.tolist()\n",
    "cols.insert(0, cols.pop(cols.index('DateTime')))\n",
    "fomc_data = fomc_data[cols]\n",
    "\n",
    "# ???? hft_data ? DateAndTime\n",
    "hft_data['DateAndTime'] = pd.to_datetime(hft_data['DateAndTime'])\n",
    "\n",
    "# ??????????\n",
    "futures_columns = [\n",
    "    'FF1_Price','FF1_Volume','FF2_Price','FF2_Volume',\n",
    "    'ED2_Price','ED2_Volume','ED3_Price','ED3_Volume',\n",
    "    'ED4_Price','ED4_Volume','2yr_price','2yr_Volume',\n",
    "    '5yr_price','5yr_Volume','10yr_price','10yr_Volume'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Emini Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and combine E-mini S&P 500 futures minute data\n",
    "\n",
    "data_root = DATA_ROOT\n",
    "es_paths = [\n",
    "    RAW_ROOT / 'ES_1min_97to22.csv',\n",
    "    RAW_ROOT / 'ES_1min_22to25.csv',\n",
    "]\n",
    "\n",
    "emini_parts = []\n",
    "for path in es_paths:\n",
    "    df = pd.read_csv(path)\n",
    "    df['DateAndTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "    df = df[['DateAndTime', 'Close', 'Volume', 'Tick Count']].rename(\n",
    "        columns={\n",
    "            'Close': 'Emini_Price',\n",
    "            'Volume': 'Emini_Volume',\n",
    "            'Tick Count': 'Emini_Count',\n",
    "        }\n",
    "    )\n",
    "    emini_parts.append(df)\n",
    "\n",
    "emini_hft = (\n",
    "    pd.concat(emini_parts, axis=0)\n",
    "    .sort_values('DateAndTime')\n",
    "    .drop_duplicates('DateAndTime', keep='last')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "for col in ['Emini_Price', 'Emini_Volume', 'Emini_Count']:\n",
    "    emini_hft[col] = pd.to_numeric(emini_hft[col], errors='coerce')\n",
    "\n",
    "emini_hft = emini_hft.dropna(subset=['DateAndTime']).sort_values('DateAndTime').reset_index(drop=True)\n",
    "\n",
    "filled = []\n",
    "for day, grp in emini_hft.groupby(emini_hft['DateAndTime'].dt.normalize(), sort=True):\n",
    "    grp = grp.sort_values('DateAndTime').set_index('DateAndTime')\n",
    "    full_index = pd.date_range(day, day + pd.Timedelta(days=1) - pd.Timedelta(minutes=1), freq='T')\n",
    "    grp = grp.reindex(full_index)\n",
    "    inserted = grp['Emini_Price'].isna()\n",
    "    grp['Emini_Price'] = grp['Emini_Price'].ffill()\n",
    "    grp.loc[inserted & grp['Emini_Price'].notna(), ['Emini_Volume', 'Emini_Count']] = 0.0\n",
    "    grp[['Emini_Volume', 'Emini_Count']] = grp[['Emini_Volume', 'Emini_Count']].fillna(0.0)\n",
    "    grp = grp.dropna(subset=['Emini_Price'])\n",
    "    grp = grp.reset_index().rename(columns={'index': 'DateAndTime'})\n",
    "    filled.append(grp)\n",
    "\n",
    "emini_hft = pd.concat(filled, ignore_index=True).sort_values('DateAndTime').reset_index(drop=True)\n",
    "\n",
    "emini_hft.to_csv(data_root / 'Emini_HFT.csv', index=False)\n",
    "print('E-mini dataset prepared:', emini_hft['DateAndTime'].min(), '->', emini_hft['DateAndTime'].max(), 'rows:', len(emini_hft))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Periods ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "PDATA_ROOT = DATA_ROOT / 'periods'\n",
    "# ===== Period boundaries =====\n",
    "pre_end    = pd.Timestamp('2008-12-31')\n",
    "zlb_start  = pd.Timestamp('2009-01-01')\n",
    "zlb_end    = pd.Timestamp('2015-11-30')\n",
    "post_start = pd.Timestamp('2015-12-01')\n",
    "\n",
    "def split_and_cache(hft_data, emini_hft, fomc_data, output_dir, *,\n",
    "                    force: bool = False,\n",
    "                    src_files: dict | None = None):\n",
    "    \"\"\"Split datasets by ZLB periods, caching results on disk.\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    outs = {\n",
    "        \"pre_zlb_hft\":    output_dir / \"pre_zlb_hft.csv\",\n",
    "        \"zlb_hft\":        output_dir / \"zlb_hft.csv\",\n",
    "        \"post_zlb_hft\":   output_dir / \"post_zlb_hft.csv\",\n",
    "        \"pre_zlb_emini\":  output_dir / \"pre_zlb_emini.csv\",\n",
    "        \"zlb_emini\":      output_dir / \"zlb_emini.csv\",\n",
    "        \"post_zlb_emini\": output_dir / \"post_zlb_emini.csv\",\n",
    "        \"pre_zlb_fomc\":   output_dir / \"pre_zlb_fomc.csv\",\n",
    "        \"zlb_fomc\":       output_dir / \"zlb_fomc.csv\",\n",
    "        \"post_zlb_fomc\":  output_dir / \"post_zlb_fomc.csv\",\n",
    "    }\n",
    "    parse_dates = {\n",
    "        key: ['DateAndTime'] for key in outs if 'fomc' not in key\n",
    "    }\n",
    "    parse_dates.update({\n",
    "        'pre_zlb_fomc': ['DateTime'],\n",
    "        'zlb_fomc': ['DateTime'],\n",
    "        'post_zlb_fomc': ['DateTime'],\n",
    "    })\n",
    "\n",
    "    def all_exist() -> bool:\n",
    "        return all(path.exists() for path in outs.values())\n",
    "\n",
    "    def outputs_newer_than_sources() -> bool:\n",
    "        if not src_files:\n",
    "            return True\n",
    "        try:\n",
    "            newest_src = max(Path(p).stat().st_mtime for p in src_files.values())\n",
    "            oldest_out = min(Path(p).stat().st_mtime for p in outs.values())\n",
    "            return oldest_out >= newest_src\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "\n",
    "    if all_exist() and not force and outputs_newer_than_sources():\n",
    "        return {\n",
    "            key: pd.read_csv(path, parse_dates=parse_dates.get(key))\n",
    "            for key, path in outs.items()\n",
    "        }\n",
    "\n",
    "    pre_zlb_hft    = hft_data[hft_data['DateAndTime'] <= pre_end]\n",
    "    zlb_hft        = hft_data[(hft_data['DateAndTime'] >= zlb_start) & (hft_data['DateAndTime'] <= zlb_end)]\n",
    "    post_zlb_hft   = hft_data[hft_data['DateAndTime'] >= post_start]\n",
    "\n",
    "    pre_zlb_emini  = emini_hft[emini_hft['DateAndTime'] <= pre_end]\n",
    "    zlb_emini      = emini_hft[(emini_hft['DateAndTime'] >= zlb_start) & (emini_hft['DateAndTime'] <= zlb_end)]\n",
    "    post_zlb_emini = emini_hft[emini_hft['DateAndTime'] >= post_start]\n",
    "\n",
    "    def _split_fomc(df: pd.DataFrame):\n",
    "        return (\n",
    "            df[df['DateTime'] <= pre_end],\n",
    "            df[(df['DateTime'] >= zlb_start) & (df['DateTime'] <= zlb_end)],\n",
    "            df[df['DateTime'] >= post_start],\n",
    "        )\n",
    "\n",
    "    pre_zlb_fomc, zlb_fomc, post_zlb_fomc = _split_fomc(fomc_data)\n",
    "\n",
    "    for key, df in {\n",
    "        \"pre_zlb_hft\": pre_zlb_hft,\n",
    "        \"zlb_hft\": zlb_hft,\n",
    "        \"post_zlb_hft\": post_zlb_hft,\n",
    "        \"pre_zlb_emini\": pre_zlb_emini,\n",
    "        \"zlb_emini\": zlb_emini,\n",
    "        \"post_zlb_emini\": post_zlb_emini,\n",
    "        \"pre_zlb_fomc\": pre_zlb_fomc,\n",
    "        \"zlb_fomc\": zlb_fomc,\n",
    "        \"post_zlb_fomc\": post_zlb_fomc,\n",
    "    }.items():\n",
    "        df.to_csv(outs[key], index=False)\n",
    "\n",
    "    return {\n",
    "        \"pre_zlb_hft\":     pre_zlb_hft,\n",
    "        \"zlb_hft\":         zlb_hft,\n",
    "        \"post_zlb_hft\":    post_zlb_hft,\n",
    "        \"pre_zlb_emini\":   pre_zlb_emini,\n",
    "        \"zlb_emini\":       zlb_emini,\n",
    "        \"post_zlb_emini\":  post_zlb_emini,\n",
    "        \"pre_zlb_fomc\":    pre_zlb_fomc,\n",
    "        \"zlb_fomc\":        zlb_fomc,\n",
    "        \"post_zlb_fomc\":   post_zlb_fomc,\n",
    "    }\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and cache datasets\n",
    "splits = split_and_cache(\n",
    "    hft_data,\n",
    "    emini_hft,\n",
    "    fomc_data,\n",
    "    DATA_ROOT / 'periods',\n",
    "    src_files={\n",
    "        'hft': RAW_ROOT / 'HFT_IRF.csv',\n",
    "        'emini_part1': RAW_ROOT / 'ES_1min_97to22.csv',\n",
    "        'emini_part2': RAW_ROOT / 'ES_1min_22to25.csv',\n",
    "        'fomc': RAW_ROOT / 'FOMC_FFR.csv',\n",
    "    },\n",
    "    force=False,\n",
    ")\n",
    "pre_zlb_hft = splits['pre_zlb_hft']\n",
    "zlb_hft = splits['zlb_hft']\n",
    "post_zlb_hft = splits['post_zlb_hft']\n",
    "pre_zlb_emini = splits['pre_zlb_emini']\n",
    "zlb_emini = splits['zlb_emini']\n",
    "post_zlb_emini = splits['post_zlb_emini']\n",
    "pre_zlb_fomc = splits['pre_zlb_fomc']\n",
    "zlb_fomc = splits['zlb_fomc']\n",
    "post_zlb_fomc = splits['post_zlb_fomc']\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setups ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cumulative_return(df, asset_col, baseline_price):\n",
    "    \"\"\"\n",
    "    standardize the asset price series by calculating cumulative returns\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['Return'] = df[asset_col].pct_change(fill_method=None)\n",
    "    df['Return'] = df['Return'].replace([np.inf, -np.inf], np.nan).fillna(0.0) \n",
    "    df['CumReturn'] = (df[asset_col] / baseline_price) - 1\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom tick formatter for the x-axis\n",
    "def format_tick(t):\n",
    "    #   Set a reference start time to format the ticks\n",
    "    ref = pd.Timestamp(\"2000-01-01 09:30\")\n",
    "    tick_time = ref + pd.Timedelta(minutes=t)\n",
    "    # Determine which day the tick belongs to based on its value\n",
    "    if t < 1440:\n",
    "        return f\"Day-1 {tick_time.strftime('%H:%M')}\"\n",
    "    elif t < 2880:\n",
    "        return f\"Day0 {tick_time.strftime('%H:%M')}\"\n",
    "    else:\n",
    "        return f\"Day+1 {tick_time.strftime('%H:%M')}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Descriptive Statistics Framework  ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter, MultipleLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "DATA_ROOT = Path('../data')\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _t_sf(t_value, df):\n",
    "    try:\n",
    "        from scipy import stats\n",
    "        return float(stats.t.sf(t_value, df))\n",
    "    except Exception:\n",
    "        if math.isinf(t_value):\n",
    "            return 0.0\n",
    "        return 0.5 * math.erfc(t_value / math.sqrt(2.0))\n",
    "\n",
    "\n",
    "def _t_cdf(t_value, df):\n",
    "    try:\n",
    "        from scipy import stats\n",
    "        return float(stats.t.cdf(t_value, df))\n",
    "    except Exception:\n",
    "        if math.isinf(t_value):\n",
    "            return 0.0 if t_value < 0 else 1.0\n",
    "        return 0.5 * (1.0 + math.erf(t_value / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "def _compute_t_summary(samples):\n",
    "    arr = np.asarray(samples, dtype=float)\n",
    "    arr = arr[~np.isnan(arr)]\n",
    "    n = arr.size\n",
    "    if n == 0:\n",
    "        return {'mean': np.nan, 't': np.nan, 'p_two': np.nan, 'n': 0}\n",
    "    mean = float(arr.mean())\n",
    "    if n < 2:\n",
    "        return {'mean': mean, 't': np.nan, 'p_two': np.nan, 'n': int(n)}\n",
    "    std = arr.std(ddof=1)\n",
    "    if std == 0:\n",
    "        t_stat = math.inf if mean > 0 else (-math.inf if mean < 0 else 0.0)\n",
    "        two_sided = 0.0 if mean != 0 else 1.0\n",
    "        return {'mean': mean, 't': t_stat, 'p_two': two_sided, 'n': int(n)}\n",
    "    t_stat = mean / (std / math.sqrt(n))\n",
    "    two_sided = 2.0 * _t_sf(abs(t_stat), n - 1)\n",
    "    return {'mean': mean, 't': t_stat, 'p_two': two_sided, 'n': int(n)}\n",
    "\n",
    "\n",
    "def _format_cell(value, pvalue, direction=None, value_fmt=\",.0f\", p_fmt=\".3f\"):\n",
    "    if pd.isna(value):\n",
    "        value_str = 'NA'\n",
    "    else:\n",
    "        value_str = format(value, value_fmt)\n",
    "    if pd.isna(pvalue):\n",
    "        p_str = 'p=NA'\n",
    "    else:\n",
    "        if pvalue < 0.001:\n",
    "            p_str = 'p<0.001'\n",
    "        else:\n",
    "            p_str = f\"p={pvalue:{p_fmt}}\"\n",
    "    if direction:\n",
    "        if direction == '>':\n",
    "            p_str = f\"{p_str} (>0)\"\n",
    "        elif direction == '<':\n",
    "            p_str = f\"{p_str} (<0)\"\n",
    "        elif direction == '=':\n",
    "            p_str = f\"{p_str} (=0)\"\n",
    "    return f\"{value_str}\\n{p_str}\"\n",
    "\n",
    "\n",
    "def _latex_escape(text):\n",
    "    replacements = {\n",
    "        '_': r'\\\\_',\n",
    "        '%': r'\\\\%',\n",
    "        '&': r'\\\\&',\n",
    "    }\n",
    "    out = str(text)\n",
    "    for key, val in replacements.items():\n",
    "        out = out.replace(key, val)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _format_cell_for_latex(cell):\n",
    "    if isinstance(cell, str) and '\\\\n' in cell:\n",
    "        top, bottom = cell.split('\\\\n', 1)\n",
    "        return (\n",
    "            r'\\\\begin{tabular}[c]{@{}c@{}}'\n",
    "            + _latex_escape(top)\n",
    "            + r'\\\\\\\\scriptsize{'\n",
    "            + _latex_escape(bottom)\n",
    "            + r'}\\\\end{tabular}'\n",
    "        )\n",
    "    return _latex_escape(cell)\n",
    "\n",
    "\n",
    "def _time_window_mask(tod_array, window_radius, announcement_tod):\n",
    "    arr = np.asarray(tod_array, dtype=int)\n",
    "    radius = abs(int(window_radius))\n",
    "    if radius >= 720:\n",
    "        return np.ones_like(arr, dtype=bool)\n",
    "    lower = (announcement_tod - radius) % 1440\n",
    "    upper = (announcement_tod + radius) % 1440\n",
    "    if lower <= upper:\n",
    "        return (arr >= lower) & (arr <= upper)\n",
    "    return (arr >= lower) | (arr <= upper)\n",
    "\n",
    "\n",
    "def _prepare_volume_long(vol_input):\n",
    "    if isinstance(vol_input, pd.DataFrame):\n",
    "        df_wide = vol_input.copy()\n",
    "        df_wide.index.name = 'MinuteFromStart'\n",
    "        columns = list(df_wide.columns)\n",
    "        if isinstance(df_wide.columns, pd.MultiIndex):\n",
    "            columns = ['_'.join(str(part) for part in tup if str(part)) for tup in columns]\n",
    "        columns = [str(col) for col in columns]\n",
    "        if pd.Index(columns).duplicated().any():\n",
    "            counts = {}\n",
    "            adjusted = []\n",
    "            for col in columns:\n",
    "                counts[col] = counts.get(col, 0) + 1\n",
    "                if counts[col] == 1:\n",
    "                    adjusted.append(col)\n",
    "                else:\n",
    "                    adjusted.append(f\"{col}__{counts[col]}\")\n",
    "            columns = adjusted\n",
    "        df_wide.columns = columns\n",
    "        long = df_wide.stack(dropna=False).reset_index()\n",
    "        long.columns = ['MinuteFromStart', 'Event', 'Volume']\n",
    "    else:\n",
    "        s = pd.Series(vol_input, copy=True)\n",
    "        s.index.name = 'MinuteFromStart'\n",
    "        long = s.rename('Volume').reset_index()\n",
    "        long['Event'] = 0\n",
    "    long['MinuteFromStart'] = pd.to_numeric(long['MinuteFromStart'], errors='coerce')\n",
    "    long['Volume'] = pd.to_numeric(long['Volume'], errors='coerce')\n",
    "    long = long.dropna(subset=['MinuteFromStart'])\n",
    "    long['MinuteInt'] = np.rint(long['MinuteFromStart']).astype(int)\n",
    "    long['tod'] = (long['MinuteInt'] % 1440).astype(int)\n",
    "    return long\n",
    "\n",
    "\n",
    "def _describe_vector(values):\n",
    "    arr = np.asarray(values, dtype=float)\n",
    "    arr = arr[~np.isnan(arr)]\n",
    "    n = arr.size\n",
    "    if n == 0:\n",
    "        return {\n",
    "            'Mean': np.nan,\n",
    "            'St. Dev': np.nan,\n",
    "            'P25': np.nan,\n",
    "            'P50': np.nan,\n",
    "            'P75': np.nan,\n",
    "            'No. Obs': 0,\n",
    "        }\n",
    "    return {\n",
    "        'Mean': float(arr.mean()),\n",
    "        'St. Dev': float(arr.std(ddof=1)) if n > 1 else 0.0,\n",
    "        'P25': float(np.percentile(arr, 25)),\n",
    "        'P50': float(np.percentile(arr, 50)),\n",
    "        'P75': float(np.percentile(arr, 75)),\n",
    "        'No. Obs': int(n),\n",
    "    }\n",
    "\n",
    "\n",
    "def _format_number(value, decimals):\n",
    "    if pd.isna(value):\n",
    "        return ''\n",
    "    return f\"{value:,.{decimals}f}\"\n",
    "\n",
    "\n",
    "\n",
    "def _write_latex_table_multi(df, caption, label, filepath=None, decimals=None, default_decimals=0):\n",
    "    decimals = decimals or {}\n",
    "    if not isinstance(df.columns, pd.MultiIndex):\n",
    "        df = df.copy()\n",
    "        df.columns = pd.MultiIndex.from_product([df.columns, ['Value']])\n",
    "    top_labels = []\n",
    "    for lbl in df.columns.get_level_values(0):\n",
    "        if lbl not in top_labels:\n",
    "            top_labels.append(lbl)\n",
    "    metrics_per_label = {lbl: [col[1] for col in df.columns if col[0] == lbl] for lbl in top_labels}\n",
    "    lines = [\n",
    "        r'\\begin{table}[!htbp]\\centering',\n",
    "        f'\\\\caption{{{caption}}}',\n",
    "        f'\\\\label{{{label}}}',\n",
    "        r'\\begin{tabular}{l' + 'c' * df.shape[1] + '}',\n",
    "        r'\\hline',\n",
    "    ]\n",
    "    header_top = ['']\n",
    "    for lbl in top_labels:\n",
    "        span = len(metrics_per_label[lbl])\n",
    "        header_top.append(f'\\\\multicolumn{{{span}}}{{c}}{{{_latex_escape(lbl)}}}')\n",
    "    lines.append(' & '.join(header_top) + ' \\\\')\n",
    "    header_bottom = ['']\n",
    "    for lbl in top_labels:\n",
    "        for metric in metrics_per_label[lbl]:\n",
    "            header_bottom.append(_latex_escape(metric))\n",
    "    lines.append(' & '.join(header_bottom) + ' \\\\')\n",
    "    lines.append(r'\\hline')\n",
    "    for idx in df.index:\n",
    "        row = [_latex_escape(str(idx))]\n",
    "        for lbl in top_labels:\n",
    "            for metric in metrics_per_label[lbl]:\n",
    "                value = df.loc[idx, (lbl, metric)]\n",
    "                decimals_here = decimals.get(metric, default_decimals)\n",
    "                row.append(_format_number(value, decimals_here))\n",
    "        lines.append(' & '.join(row) + ' \\\\')\n",
    "    lines.extend([\n",
    "        r'\\hline',\n",
    "        r'\\end{tabular}',\n",
    "        r'\\end{table}',\n",
    "    ])\n",
    "    content = '\\n'.join(lines)\n",
    "    if filepath is not None:\n",
    "        Path(filepath).write_text(content, encoding='utf-8')\n",
    "    return content\n",
    "\n",
    "\n",
    "def summarize_fomc_daily_volume(\n",
    "    vol_input,\n",
    "    k,\n",
    "    window_days=2,\n",
    "    make_tables=False,\n",
    "    out_prefix=None\n",
    "):\n",
    "    \"\"\"Compute daily FOMC week volume statistics and return a formatted table.\"\"\"\n",
    "    long = _prepare_volume_long(vol_input)\n",
    "    k_int = int(k)\n",
    "    window_days = int(window_days)\n",
    "    focus_days = list(range(-window_days, window_days + 1))\n",
    "    long['DayIndex'] = (long['MinuteInt'] // 1440) - k_int\n",
    "    filtered = long[long['DayIndex'].isin(focus_days)].copy()\n",
    "    if filtered.empty:\n",
    "        empty_columns = pd.MultiIndex.from_product([\n",
    "            [f'FOMC_week_Day ({d:+d})' for d in focus_days],\n",
    "            ['Mean', 'St. Dev', 'P25', 'P50', 'P75', 'No. Obs'],\n",
    "        ])\n",
    "        empty_table = pd.DataFrame(index=['Avg Daily Volume', 'Diff_Vol (Ann - Day)', '# Obs'], columns=empty_columns)\n",
    "        return {\n",
    "            'table': empty_table,\n",
    "            'daily_totals': pd.DataFrame(),\n",
    "            'per_day_stats': {},\n",
    "            'diff_means': {},\n",
    "        }\n",
    "    daily_totals = (\n",
    "        filtered.groupby(['Event', 'DayIndex'])['Volume']\n",
    "        .sum(min_count=1)\n",
    "        .unstack('DayIndex')\n",
    "        .reindex(columns=focus_days)\n",
    "    )\n",
    "    column_labels = {\n",
    "        -2: 'FOMC_week_Day (-2)',\n",
    "        -1: 'FOMC_week_Day (-1)',\n",
    "        0: 'FOMC_week_Day (0)',\n",
    "        1: 'FOMC_week_Day (+1)',\n",
    "        2: 'FOMC_week_Day (+2)',\n",
    "    }\n",
    "    metrics = ['Mean', 'St. Dev', 'P25', 'P50', 'P75', 'No. Obs']\n",
    "    columns = []\n",
    "    labels_in_use = []\n",
    "    for day in focus_days:\n",
    "        label = column_labels.get(day, f'FOMC_week_Day ({day:+d})')\n",
    "        labels_in_use.append(label)\n",
    "        for metric in metrics:\n",
    "            columns.append((label, metric))\n",
    "    stats_table = pd.DataFrame(np.nan, index=['Avg Daily Volume', 'Diff_Vol (Ann - Day)', '# Obs'], columns=pd.MultiIndex.from_tuples(columns))\n",
    "    per_day_stats = {}\n",
    "    diff_means = {}\n",
    "    base_series = daily_totals[0] if 0 in daily_totals.columns else pd.Series(dtype=float)\n",
    "    for day, label in zip(focus_days, labels_in_use):\n",
    "        series = daily_totals[day] if day in daily_totals.columns else pd.Series(dtype=float)\n",
    "        stats = _describe_vector(series)\n",
    "        per_day_stats[day] = stats\n",
    "        for metric in metrics:\n",
    "            stats_table.loc['Avg Daily Volume', (label, metric)] = stats.get(metric, np.nan)\n",
    "        diff_value = np.nan\n",
    "        if day == 0:\n",
    "            diff_value = 0.0 if stats['No. Obs'] > 0 else np.nan\n",
    "        elif not base_series.empty:\n",
    "            diffs = (base_series - series).dropna()\n",
    "            diff_value = float(diffs.mean()) if not diffs.empty else np.nan\n",
    "        diff_means[day] = diff_value\n",
    "        stats_table.loc['Diff_Vol (Ann - Day)', (label, 'Mean')] = diff_value\n",
    "        stats_table.loc['# Obs', (label, 'Mean')] = stats.get('No. Obs', np.nan)\n",
    "    return {\n",
    "        'table': stats_table,\n",
    "        'daily_totals': daily_totals,\n",
    "        'per_day_stats': per_day_stats,\n",
    "        'diff_means': diff_means,\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_volume_periodicity(\n",
    "    vol_input,\n",
    "    k,\n",
    "    window_list=(15, 30, 60, 120, 720),\n",
    "    market_open_min=570,\n",
    "    market_close_min=960,\n",
    "    exclude_minutes=30,\n",
    "    make_tables=False,\n",
    "    out_prefix=None\n",
    "):\n",
    "    \"\"\"Compute per-minute intraday window statistics for announcement and non-announcement days.\"\"\"\n",
    "    long = _prepare_volume_long(vol_input)\n",
    "    k_int = int(k)\n",
    "    window_list = [abs(int(w)) for w in window_list]\n",
    "    long['DayIndex'] = (long['MinuteInt'] // 1440) - k_int\n",
    "    open_start = max(0, market_open_min - exclude_minutes)\n",
    "    open_end = min(1440, market_open_min + exclude_minutes)\n",
    "    close_start = max(0, market_close_min - exclude_minutes)\n",
    "    close_end = min(1440, market_close_min + exclude_minutes)\n",
    "    excl_mask = (\n",
    "        ((long['tod'] >= open_start) & (long['tod'] < open_end))\n",
    "        | ((long['tod'] >= close_start) & (long['tod'] < close_end))\n",
    "    )\n",
    "    use = long.loc[~excl_mask].copy()\n",
    "    use = use.dropna(subset=['Volume'])\n",
    "    if use.empty:\n",
    "        empty_columns = pd.MultiIndex.from_product([\n",
    "            [f'±{w}m' for w in window_list],\n",
    "            ['Mean', 'St. Dev', 'P25', 'P50', 'P75', 'No. Obs'],\n",
    "        ])\n",
    "        empty = pd.DataFrame(index=['Ann Window Volume', 'Diff (Ann - Non)', '# Obs'], columns=empty_columns)\n",
    "        return {\n",
    "            'volume_table': empty,\n",
    "            'variance_table': empty,\n",
    "            'volume_summary': pd.DataFrame(),\n",
    "            'variance_summary': pd.DataFrame(),\n",
    "            'per_event_window_stats': pd.DataFrame(),\n",
    "        }\n",
    "    announcement_tod = 14 * 60\n",
    "    window_records = []\n",
    "    for (event, day), grp in use.groupby(['Event', 'DayIndex']):\n",
    "        vols = grp['Volume'].astype(float).to_numpy()\n",
    "        tods = grp['tod'].to_numpy()\n",
    "        for w in window_list:\n",
    "            mask = _time_window_mask(tods, w, announcement_tod)\n",
    "            if mask.any():\n",
    "                selected = vols[mask]\n",
    "                valid = selected[~np.isnan(selected)]\n",
    "                if valid.size:\n",
    "                    avg = float(valid.mean())\n",
    "                    var = float(valid.var(ddof=1)) if valid.size > 1 else np.nan\n",
    "                    count = int(valid.size)\n",
    "                else:\n",
    "                    avg = np.nan\n",
    "                    var = np.nan\n",
    "                    count = 0\n",
    "            else:\n",
    "                avg = np.nan\n",
    "                var = np.nan\n",
    "                count = 0\n",
    "            window_records.append({\n",
    "                'Event': event,\n",
    "                'DayIndex': int(day),\n",
    "                'Window': int(w),\n",
    "                'AverageVolume': avg,\n",
    "                'Variance': var,\n",
    "                'Count': count,\n",
    "            })\n",
    "    window_stats = pd.DataFrame(window_records)\n",
    "    if window_stats.empty:\n",
    "        empty_columns = pd.MultiIndex.from_product([\n",
    "            [f'±{w}m' for w in window_list],\n",
    "            ['Mean', 'St. Dev', 'P25', 'P50', 'P75', 'No. Obs'],\n",
    "        ])\n",
    "        empty = pd.DataFrame(index=['Ann Window Volume', 'Diff (Ann - Non)', '# Obs'], columns=empty_columns)\n",
    "        return {\n",
    "            'volume_table': empty,\n",
    "            'variance_table': empty,\n",
    "            'volume_summary': pd.DataFrame(),\n",
    "            'variance_summary': pd.DataFrame(),\n",
    "            'per_event_window_stats': pd.DataFrame(),\n",
    "        }\n",
    "    ann_stats = window_stats[window_stats['DayIndex'] == 0].rename(columns={'Count': 'AnnCount'})\n",
    "    non_stats = window_stats[window_stats['DayIndex'] != 0]\n",
    "    non_avg = (\n",
    "        non_stats.groupby(['Event', 'Window'])['AverageVolume']\n",
    "        .mean()\n",
    "        .reset_index(name='AverageVolume_non')\n",
    "    )\n",
    "    non_var = (\n",
    "        non_stats.groupby(['Event', 'Window'])['Variance']\n",
    "        .mean()\n",
    "        .reset_index(name='Variance_non')\n",
    "    )\n",
    "    non_days = (\n",
    "        non_stats.groupby(['Event', 'Window'])['AverageVolume']\n",
    "        .count()\n",
    "        .reset_index(name='NonDays')\n",
    "    )\n",
    "    merged = ann_stats.merge(non_avg, on=['Event', 'Window'], how='left')\n",
    "    merged = merged.merge(non_var, on=['Event', 'Window'], how='left')\n",
    "    merged = merged.merge(non_days, on=['Event', 'Window'], how='left')\n",
    "    window_label_map = {\n",
    "        15: '±15m',\n",
    "        30: '±30m',\n",
    "        60: '±1h',\n",
    "        120: '±2h',\n",
    "        720: '±12h',\n",
    "    }\n",
    "    metrics = ['Mean', 'St. Dev', 'P25', 'P50', 'P75', 'No. Obs']\n",
    "    tuples = []\n",
    "    labels_in_use = []\n",
    "    for w in window_list:\n",
    "        label = window_label_map.get(w, f'±{w}m')\n",
    "        labels_in_use.append(label)\n",
    "        for metric in metrics:\n",
    "            tuples.append((label, metric))\n",
    "    volume_table = pd.DataFrame(np.nan, index=['Ann Window Volume', 'Diff (Ann - Non)', '# Obs'], columns=pd.MultiIndex.from_tuples(tuples))\n",
    "    variance_table = volume_table.copy()\n",
    "    volume_summary_records = []\n",
    "    variance_summary_records = []\n",
    "    for w, label in zip(window_list, labels_in_use):\n",
    "        subset = merged[merged['Window'] == w]\n",
    "        ann_vals = subset['AverageVolume'].dropna()\n",
    "        diff_vals = (subset['AverageVolume'] - subset['AverageVolume_non']).dropna()\n",
    "        stats = _describe_vector(ann_vals)\n",
    "        for metric in metrics:\n",
    "            volume_table.loc['Ann Window Volume', (label, metric)] = stats.get(metric, np.nan)\n",
    "        volume_table.loc['Diff (Ann - Non)', (label, 'Mean')] = float(diff_vals.mean()) if not diff_vals.empty else np.nan\n",
    "        volume_table.loc['# Obs', (label, 'Mean')] = stats.get('No. Obs', np.nan)\n",
    "        volume_summary_records.append({\n",
    "            'window_min': w,\n",
    "            'label': label,\n",
    "            **stats,\n",
    "            'DiffMean': float(diff_vals.mean()) if not diff_vals.empty else np.nan,\n",
    "        })\n",
    "        ann_var_vals = subset['Variance'].dropna()\n",
    "        diff_var_vals = (subset['Variance'] - subset['Variance_non']).dropna()\n",
    "        var_stats = _describe_vector(ann_var_vals)\n",
    "        for metric in metrics:\n",
    "            variance_table.loc['Ann Window Volume', (label, metric)] = var_stats.get(metric, np.nan)\n",
    "        variance_table.loc['Diff (Ann - Non)', (label, 'Mean')] = float(diff_var_vals.mean()) if not diff_var_vals.empty else np.nan\n",
    "        variance_table.loc['# Obs', (label, 'Mean')] = var_stats.get('No. Obs', np.nan)\n",
    "        variance_summary_records.append({\n",
    "            'window_min': w,\n",
    "            'label': label,\n",
    "            **var_stats,\n",
    "            'DiffMean': float(diff_var_vals.mean()) if not diff_var_vals.empty else np.nan,\n",
    "        })\n",
    "    volume_summary = pd.DataFrame(volume_summary_records).set_index('window_min') if volume_summary_records else pd.DataFrame()\n",
    "    variance_summary = pd.DataFrame(variance_summary_records).set_index('window_min') if variance_summary_records else pd.DataFrame()\n",
    "    per_event_window_stats = merged[\n",
    "        ['Event', 'Window', 'AnnCount', 'NonDays', 'AverageVolume', 'AverageVolume_non', 'Variance', 'Variance_non']\n",
    "    ].copy()\n",
    "    return {\n",
    "        'volume_table': volume_table,\n",
    "        'variance_table': variance_table,\n",
    "        'volume_summary': volume_summary,\n",
    "        'variance_summary': variance_summary,\n",
    "        'per_event_window_stats': per_event_window_stats,\n",
    "    }\n",
    "\n",
    "\n",
    "def _persist_volume_outputs(asset, period, k, daily_table, intraday_volume_table, intraday_variance_table):\n",
    "    asset_str = str(asset)\n",
    "    asset_dir = DATA_ROOT / asset_str\n",
    "    asset_dir.mkdir(parents=True, exist_ok=True)\n",
    "    period_str = str(period) if period else 'period'\n",
    "    safe_period = period_str.replace(' ', '_')\n",
    "    excel_path = asset_dir / f'{asset_str}_k{k}_{safe_period}.xlsx'\n",
    "    with pd.ExcelWriter(excel_path) as writer:\n",
    "        daily_table.to_excel(writer, sheet_name='daily volume')\n",
    "        intraday_volume_table.to_excel(writer, sheet_name='intraday volume')\n",
    "        intraday_variance_table.to_excel(writer, sheet_name='intraday volume variance')\n",
    "    latex_dir = asset_dir / 'latex'\n",
    "    latex_dir.mkdir(parents=True, exist_ok=True)\n",
    "    daily_tex = _write_latex_table_multi(\n",
    "        daily_table,\n",
    "        'Average daily volumes around FOMC announcement',\n",
    "        f'tab:{asset_str}_{safe_period}_daily_volume',\n",
    "        filepath=None,\n",
    "        decimals={'Mean': 0, 'St. Dev': 0, 'P25': 0, 'P50': 0, 'P75': 0, 'No. Obs': 0},\n",
    "        default_decimals=0,\n",
    "    )\n",
    "    intraday_vol_tex = _write_latex_table_multi(\n",
    "        intraday_volume_table,\n",
    "        'Intraday announcement window volume (per-minute averages)',\n",
    "        f'tab:{asset_str}_{safe_period}_intraday_volume',\n",
    "        filepath=None,\n",
    "        decimals={'Mean': 3, 'St. Dev': 3, 'P25': 3, 'P50': 3, 'P75': 3, 'No. Obs': 0},\n",
    "        default_decimals=3,\n",
    "    )\n",
    "    intraday_var_tex = _write_latex_table_multi(\n",
    "        intraday_variance_table,\n",
    "        'Intraday announcement window variance',\n",
    "        f'tab:{asset_str}_{safe_period}_intraday_variance',\n",
    "        filepath=None,\n",
    "        decimals={'Mean': 6, 'St. Dev': 6, 'P25': 6, 'P50': 6, 'P75': 6, 'No. Obs': 0},\n",
    "        default_decimals=6,\n",
    "    )\n",
    "    latex_path = latex_dir / f'{safe_period}.tex'\n",
    "    latex_content = '\\n\\n'.join([daily_tex, intraday_vol_tex, intraday_var_tex])\n",
    "    latex_path.write_text(latex_content, encoding='utf-8')\n",
    "    return {\n",
    "        'excel_path': excel_path,\n",
    "        'latex_path': latex_path,\n",
    "        'asset_dir': asset_dir,\n",
    "        'latex_dir': latex_dir,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Return/Volume Plots Framework ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fomc_k_window(hft_input, fomc_input, k=1, asset='', period='', extra_metrics=None):\n",
    "    \"\"\"Plot cumulative returns and run volume diagnostics around FOMC events.\"\"\"\n",
    "    from pathlib import Path\n",
    "\n",
    "    extra_metrics = extra_metrics or {}\n",
    "    metric_series = {name: [] for name in extra_metrics}\n",
    "\n",
    "    fomc_list = fomc_input['DateTime']\n",
    "    all_cr_list = []\n",
    "    all_vol_list = []\n",
    "    price_col = asset + '_Price'\n",
    "    volume_col = asset + '_Volume'\n",
    "\n",
    "    window_start_reference = lambda dt: dt.normalize() - pd.Timedelta(days=k)\n",
    "    window_end_reference = lambda dt: dt.normalize() + pd.Timedelta(days=k + 1)\n",
    "\n",
    "    time_length_hours = 24 * (2 * k + 1)\n",
    "    total_minutes = int(time_length_hours * 60)\n",
    "\n",
    "    for fomc_dt in fomc_list:\n",
    "        window_start = window_start_reference(fomc_dt)\n",
    "        window_end = window_end_reference(fomc_dt)\n",
    "\n",
    "        event_df = hft_input[(hft_input['DateAndTime'] >= window_start) & (hft_input['DateAndTime'] < window_end)].copy()\n",
    "        if event_df.empty:\n",
    "            continue\n",
    "\n",
    "        baseline_row = event_df.iloc[(event_df['DateAndTime'] - window_start).abs().argmin()]\n",
    "        baseline_price = baseline_row[price_col]\n",
    "\n",
    "        event_df = calculate_cumulative_return(event_df, price_col, baseline_price)\n",
    "        event_df['MinuteFromStart'] = (event_df['DateAndTime'] - window_start).dt.total_seconds() / 60\n",
    "        event_df.set_index('MinuteFromStart', inplace=True)\n",
    "\n",
    "        all_cr_list.append(event_df['CumReturn'])\n",
    "        all_vol_list.append(event_df[volume_col])\n",
    "        for metric_name, column in extra_metrics.items():\n",
    "            if column in event_df.columns:\n",
    "                metric_series[metric_name].append(event_df[column])\n",
    "\n",
    "    if not all_cr_list:\n",
    "        print('No valid FOMC events found!')\n",
    "        return None\n",
    "\n",
    "    merged_cr = pd.concat(all_cr_list, axis=1)\n",
    "    avg_cr = merged_cr.mean(axis=1)\n",
    "    std_cr = merged_cr.std(axis=1)\n",
    "    n = merged_cr.shape[1]\n",
    "\n",
    "    ci_upper = avg_cr + 1.96 * std_cr / np.sqrt(n)\n",
    "    ci_lower = avg_cr - 1.96 * std_cr / np.sqrt(n)\n",
    "\n",
    "    avg_cr = avg_cr.sort_index()\n",
    "    ci_upper = ci_upper.sort_index()\n",
    "    ci_lower = ci_lower.sort_index()\n",
    "    full_minutes = np.arange(0, total_minutes + 1)\n",
    "    avg_cr = avg_cr.reindex(full_minutes, method='nearest')\n",
    "    ci_upper = ci_upper.reindex(full_minutes, method='nearest')\n",
    "    ci_lower = ci_lower.reindex(full_minutes, method='nearest')\n",
    "\n",
    "    avg_cr_bps = avg_cr * 10000\n",
    "    ci_upper_bps = ci_upper * 10000\n",
    "    ci_lower_bps = ci_lower * 10000\n",
    "\n",
    "    merged_vol = pd.concat(all_vol_list, axis=1)\n",
    "    avg_vol = merged_vol.mean(axis=1)\n",
    "    avg_vol = avg_vol.sort_index().reindex(full_minutes, method='nearest')\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "    ax1.plot(full_minutes, avg_cr_bps, color='blue', linewidth=1, label='Avg CumReturn (bps)')\n",
    "    ax1.fill_between(full_minutes, ci_lower_bps, ci_upper_bps, color='blue', alpha=0.2, label='95% CI')\n",
    "    ax1.set_ylabel('Cumulative Return (bps)', fontsize=12)\n",
    "    ax1.set_xlabel('Time', fontsize=12)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.bar(full_minutes, avg_vol, width=1.0, color='gray', alpha=0.3, label='Avg Volume')\n",
    "    ax2.set_ylabel('Volume', fontsize=12)\n",
    "\n",
    "    announcement_minute = int(k * 1440 + 840)\n",
    "    ax1.axvline(x=announcement_minute, color='red', linestyle='--', linewidth=0.5, label='FOMC Announcement')\n",
    "\n",
    "    num_days = 2 * k + 1\n",
    "    for i in range(1, num_days):\n",
    "        day_boundary = i * 1440\n",
    "        ax1.axvline(x=day_boundary, color='black', linewidth=1)\n",
    "\n",
    "    ax1.xaxis.set_major_locator(MultipleLocator(120))\n",
    "    ax1.xaxis.set_minor_locator(MultipleLocator(60))\n",
    "    ax1.tick_params(axis='x', which='major', length=10, rotation=45)\n",
    "    ax1.tick_params(axis='x', which='minor', length=5)\n",
    "\n",
    "    def time_formatter(x, pos):\n",
    "        t = x % 1440\n",
    "        hours = int(t // 60)\n",
    "        minutes = int(t % 60)\n",
    "        return f'{hours:02d}:{minutes:02d}'\n",
    "\n",
    "    ax1.xaxis.set_major_formatter(FuncFormatter(time_formatter))\n",
    "\n",
    "    ax_top = ax1.twiny()\n",
    "    ax_top.set_xlim(ax1.get_xlim())\n",
    "    day_centers = [i * 1440 + 720 for i in range(num_days)]\n",
    "    day_labels = [f'Day {i - k}' for i in range(num_days)]\n",
    "    ax_top.set_xticks(day_centers)\n",
    "    ax_top.set_xticklabels(day_labels)\n",
    "    ax_top.set_xlabel('Day', fontsize=12)\n",
    "\n",
    "    ax1.set_xlim(0, total_minutes)\n",
    "\n",
    "    fig.suptitle(f'FOMC +/-{k} Days: {price_col} & {volume_col} in {period}_Period', fontsize=16)\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax1.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    asset_str = str(asset) if asset else 'asset'\n",
    "    period_str = str(period) if period else 'period'\n",
    "    safe_period = period_str.replace(' ', '_')\n",
    "    asset_dir = DATA_ROOT / asset_str\n",
    "    asset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fig_path = asset_dir / f'fomc_window_k{k}_{safe_period}.png'\n",
    "    fig.savefig(fig_path, dpi=200, bbox_inches='tight')\n",
    "    print(f'Saved figure: {fig_path}')\n",
    "    plt.show()\n",
    "\n",
    "    cr_table = pd.DataFrame({\n",
    "        'MinuteFromStart': full_minutes,\n",
    "        'Average_CR_bps': avg_cr_bps.values,\n",
    "        'CI_Lower_bps': ci_lower_bps.values,\n",
    "        'CI_Upper_bps': ci_upper_bps.values,\n",
    "        'volume': avg_vol.values,\n",
    "    })\n",
    "    cr_out = asset_dir / f'{asset_str}_in_{k}_cr_{safe_period}.csv'\n",
    "    cr_table.to_csv(cr_out, index=False)\n",
    "    print(f'Saved CR table: {cr_out}')\n",
    "\n",
    "    daily_summary = summarize_fomc_daily_volume(\n",
    "        merged_vol,\n",
    "        k=k,\n",
    "        window_days=2,\n",
    "        make_tables=False,\n",
    "    )\n",
    "    intraday_summary = analyze_volume_periodicity(\n",
    "        merged_vol,\n",
    "        k=k,\n",
    "        window_list=(15, 30, 60, 120, 720),\n",
    "        make_tables=False,\n",
    "    )\n",
    "\n",
    "    persist_info = _persist_volume_outputs(\n",
    "        asset_str,\n",
    "        period_str,\n",
    "        k,\n",
    "        daily_summary['table'],\n",
    "        intraday_summary['volume_table'],\n",
    "        intraday_summary['variance_table'],\n",
    "    )\n",
    "    print(f\"Saved summary workbook: {persist_info['excel_path']}\")\n",
    "    print(f\"Saved LaTeX bundle: {persist_info['latex_path']}\")\n",
    "\n",
    "    extra_exports = {}\n",
    "    if extra_metrics:\n",
    "        for metric_name, series_list in metric_series.items():\n",
    "            if not series_list:\n",
    "                continue\n",
    "            merged_metric = pd.concat(series_list, axis=1)\n",
    "            metric_period = f\"{period_str}_{metric_name}\".strip('_')\n",
    "            metric_daily = summarize_fomc_daily_volume(\n",
    "                merged_metric,\n",
    "                k=k,\n",
    "                window_days=2,\n",
    "                make_tables=False,\n",
    "            )\n",
    "            metric_intraday = analyze_volume_periodicity(\n",
    "                merged_metric,\n",
    "                k=k,\n",
    "                window_list=(15, 30, 60, 120, 720),\n",
    "                make_tables=False,\n",
    "            )\n",
    "            metric_export = _persist_volume_outputs(\n",
    "                asset_str,\n",
    "                metric_period,\n",
    "                k,\n",
    "                metric_daily['table'],\n",
    "                metric_intraday['volume_table'],\n",
    "                metric_intraday['variance_table'],\n",
    "            )\n",
    "            print(f\"Saved extras ({metric_name}) workbook: {metric_export['excel_path']}\")\n",
    "            print(f\"Saved extras ({metric_name}) LaTeX: {metric_export['latex_path']}\")\n",
    "            extra_exports[metric_name] = {\n",
    "                'daily_summary': metric_daily,\n",
    "                'intraday_summary': metric_intraday,\n",
    "                'export_paths': metric_export,\n",
    "            }\n",
    "\n",
    "    return {\n",
    "        'figure_path': fig_path,\n",
    "        'cr_table_path': cr_out,\n",
    "        'daily_summary': daily_summary,\n",
    "        'intraday_summary': intraday_summary,\n",
    "        'export_paths': persist_info,\n",
    "        'extra_metrics': extra_exports,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Example Plot ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Load Data ==========\n",
    "\n",
    "### ========== Example ==========\n",
    "\n",
    "```python\n",
    "plot_fomc_k_window(hft_input=hft_data, fomc_input=fomc_data,k=1, asset='FF1',period='Total')\n",
    "plot_fomc_k_window(hft_input=hft_data, fomc_input=fomc_data,k=1, asset='ED3',period='Total')\n",
    "plot_fomc_k_window(hft_input=hft_data, fomc_input=fomc_data,k=1, asset='2yr',period='Total')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOMC Day's Plot Examples\n",
    "plot_fomc_k_window(hft_input=hft_data, fomc_input=fomc_data,k=0, asset='ED3',period='Total')\n",
    "plot_fomc_k_window(\n",
    "    hft_input=emini_hft,\n",
    "    fomc_input=fomc_data,\n",
    "    k=2,\n",
    "    asset='Emini',\n",
    "    period='Total',\n",
    "    extra_metrics={'Count': 'Emini_Count'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Summary Tables ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate summary tables across assets\n",
    "\n",
    "def load_asset_tables(asset, period='Total', k=2):\n",
    "    safe_period = str(period).replace(' ', '_')\n",
    "    excel_path = DATA_ROOT / asset / f'{asset}_k{k}_{safe_period}.xlsx'\n",
    "    if not excel_path.exists():\n",
    "        raise FileNotFoundError(f\"Summary workbook not found: {excel_path}\")\n",
    "    daily = pd.read_excel(excel_path, sheet_name='daily volume', header=[0, 1], index_col=0)\n",
    "    intraday = pd.read_excel(excel_path, sheet_name='intraday volume', header=[0, 1], index_col=0)\n",
    "    daily.index.name = 'Statistic'\n",
    "    intraday.index.name = 'Statistic'\n",
    "    return daily, intraday\n",
    "\n",
    "def aggregate_assets(assets, period='Total', k=2):\n",
    "    daily_tables = []\n",
    "    intraday_tables = []\n",
    "    for asset in assets:\n",
    "        daily, intraday = load_asset_tables(asset, period=period, k=k)\n",
    "        daily_tables.append(daily)\n",
    "        intraday_tables.append(intraday)\n",
    "    daily_panel = pd.concat(daily_tables, keys=assets, names=['Asset', 'Statistic'])\n",
    "    intraday_panel = pd.concat(intraday_tables, keys=assets, names=['Asset', 'Statistic'])\n",
    "    return daily_panel, intraday_panel\n",
    "\n",
    "bond_assets = ['FF1', 'FF2', 'ED2', 'ED3', 'ED4', '2yr', '5yr', '10yr']\n",
    "emini_assets = ['Emini']\n",
    "\n",
    "bond_daily_total, bond_intraday_total = aggregate_assets(bond_assets, period='Total', k=2)\n",
    "emini_daily_total, emini_intraday_total = aggregate_assets(emini_assets, period='Total', k=2)\n",
    "\n",
    "table1 = pd.concat(\n",
    "    {\n",
    "        'Panel A (Bond Futures)': bond_daily_total,\n",
    "        'Panel B (E-mini Futures)': emini_daily_total,\n",
    "    },\n",
    "    names=['Panel', 'Asset', 'Statistic'],\n",
    ")\n",
    "\n",
    "table2 = pd.concat(\n",
    "    {\n",
    "        'Panel A (Bond Futures)': bond_intraday_total,\n",
    "        'Panel B (E-mini Futures)': emini_intraday_total,\n",
    "    },\n",
    "    names=['Panel', 'Asset', 'Statistic'],\n",
    ")\n",
    "\n",
    "summary_dir = DATA_ROOT / 'summary_tables'\n",
    "summary_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "table1_path = summary_dir / 'Table1_daily_volume_total.xlsx'\n",
    "table2_path = summary_dir / 'Table2_intraday_volume_total.xlsx'\n",
    "\n",
    "table1.to_excel(table1_path)\n",
    "table2.to_excel(table2_path)\n",
    "\n",
    "periods = ['Pre-ZLB', 'ZLB', 'Post-ZLB']\n",
    "table3_daily = {}\n",
    "table3_intraday = {}\n",
    "\n",
    "for period in periods:\n",
    "    bond_daily_p, bond_intraday_p = aggregate_assets(bond_assets, period=period, k=2)\n",
    "    emini_daily_p, emini_intraday_p = aggregate_assets(emini_assets, period=period, k=2)\n",
    "    table3_daily[period] = pd.concat(\n",
    "        {\n",
    "            'Panel A (Bond Futures)': bond_daily_p,\n",
    "            'Panel B (E-mini Futures)': emini_daily_p,\n",
    "        },\n",
    "        names=['Panel', 'Asset', 'Statistic'],\n",
    "    )\n",
    "    table3_intraday[period] = pd.concat(\n",
    "        {\n",
    "            'Panel A (Bond Futures)': bond_intraday_p,\n",
    "            'Panel B (E-mini Futures)': emini_intraday_p,\n",
    "        },\n",
    "        names=['Panel', 'Asset', 'Statistic'],\n",
    "    )\n",
    "\n",
    "with pd.ExcelWriter(summary_dir / 'Table3_volume_by_period.xlsx') as writer:\n",
    "    for period in periods:\n",
    "        table3_daily[period].to_excel(writer, sheet_name=f'{period}_daily')\n",
    "        table3_intraday[period].to_excel(writer, sheet_name=f'{period}_intraday')\n",
    "\n",
    "print('Table1 saved to', table1_path)\n",
    "print('Table2 saved to', table2_path)\n",
    "print('Table3 workbook saved to', summary_dir / 'Table3_volume_by_period.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual Tables and Latex Export\n",
    "import numpy as np\n",
    "\n",
    "def _short_panel_label(label):\n",
    "    if isinstance(label, str):\n",
    "        stripped = label.strip()\n",
    "        if stripped.startswith('Panel A'):\n",
    "            return 'Panel A'\n",
    "        if stripped.startswith('Panel B'):\n",
    "            return 'Panel B'\n",
    "    return str(label)\n",
    "\n",
    "def _safe_value(series, outer, metric):\n",
    "    if isinstance(series, pd.DataFrame):\n",
    "        series = series.iloc[0]\n",
    "    if isinstance(series.index, pd.MultiIndex):\n",
    "        key = (outer, metric)\n",
    "        if key in series.index:\n",
    "            return series.loc[key]\n",
    "    else:\n",
    "        key = f\"{outer} {metric}\"\n",
    "        if key in series.index:\n",
    "            return series.loc[key]\n",
    "    return np.nan\n",
    "\n",
    "def _prepare_table_layout(table, kind):\n",
    "    if not isinstance(table.columns, pd.MultiIndex):\n",
    "        raise ValueError('Expected table with MultiIndex columns')\n",
    "    if not isinstance(table.index, pd.MultiIndex):\n",
    "        table = table.copy()\n",
    "        table.index = pd.MultiIndex.from_frame(table.index.to_frame())\n",
    "    stat_level = table.index.names[-1]\n",
    "    rows = []\n",
    "    panel_counts = {}\n",
    "    asset_counts = {}\n",
    "    if kind == 'daily':\n",
    "        outer_labels = ['FOMC_week_Day (-2)', 'FOMC_week_Day (-1)', 'FOMC_week_Day (0)', 'FOMC_week_Day (+1)', 'FOMC_week_Day (+2)']\n",
    "        value_columns = ['-2', '-1', '0', '1', '2']\n",
    "        for (panel, asset), sub in table.groupby(level=table.index.names[:2], sort=False):\n",
    "            panel_short = _short_panel_label(panel)\n",
    "            try:\n",
    "                avg_series = sub.xs('Avg Daily Volume', level=stat_level)\n",
    "            except KeyError:\n",
    "                continue\n",
    "            obs_fallback = sub.xs('# Obs', level=stat_level) if '# Obs' in sub.index else None\n",
    "            row_avg = {'Panel': panel_short, 'Asset': str(asset), 'Statistic': 'Avg. Daily Volume', 'RowType': 'avg'}\n",
    "            row_std = {'Panel': panel_short, 'Asset': str(asset), 'Statistic': 'St. Dev', 'RowType': 'std'}\n",
    "            row_obs = {'Panel': panel_short, 'Asset': str(asset), 'Statistic': '# Obs', 'RowType': 'obs'}\n",
    "            for col_label, outer in zip(value_columns, outer_labels):\n",
    "                row_avg[col_label] = _safe_value(avg_series, outer, 'Mean')\n",
    "                row_std[col_label] = _safe_value(avg_series, outer, 'St. Dev')\n",
    "                obs_val = _safe_value(avg_series, outer, 'No. Obs')\n",
    "                if pd.isna(obs_val) and obs_fallback is not None:\n",
    "                    obs_val = _safe_value(obs_fallback, outer, 'Mean')\n",
    "                row_obs[col_label] = obs_val\n",
    "            for row in (row_avg, row_std, row_obs):\n",
    "                rows.append(row)\n",
    "                panel_counts[panel_short] = panel_counts.get(panel_short, 0) + 1\n",
    "                asset_key = (panel_short, str(asset))\n",
    "                asset_counts[asset_key] = asset_counts.get(asset_key, 0) + 1\n",
    "        top_header = 'FOMC Week Day'\n",
    "    else:\n",
    "        outer_labels = ['±15m', '±30m', '±1h', '±2h', '±12h']\n",
    "        value_columns = outer_labels\n",
    "        for (panel, asset), sub in table.groupby(level=table.index.names[:2], sort=False):\n",
    "            panel_short = _short_panel_label(panel)\n",
    "            try:\n",
    "                avg_series = sub.xs('Ann Window Volume', level=stat_level)\n",
    "            except KeyError:\n",
    "                continue\n",
    "            diff_series = sub.xs('Diff (Ann - Non)', level=stat_level) if 'Diff (Ann - Non)' in sub.index else None\n",
    "            obs_fallback = sub.xs('# Obs', level=stat_level) if '# Obs' in sub.index else None\n",
    "            row_avg = {'Panel': panel_short, 'Asset': str(asset), 'Statistic': 'Avg Window Volume', 'RowType': 'avg'}\n",
    "            row_diff = {'Panel': panel_short, 'Asset': str(asset), 'Statistic': 'Diff', 'RowType': 'diff'}\n",
    "            row_obs = {'Panel': panel_short, 'Asset': str(asset), 'Statistic': '# Obs', 'RowType': 'obs'}\n",
    "            for outer in outer_labels:\n",
    "                row_avg[outer] = _safe_value(avg_series, outer, 'Mean')\n",
    "                diff_val = np.nan if diff_series is None else _safe_value(diff_series, outer, 'Mean')\n",
    "                row_diff[outer] = diff_val\n",
    "                obs_val = _safe_value(avg_series, outer, 'No. Obs')\n",
    "                if pd.isna(obs_val) and obs_fallback is not None:\n",
    "                    obs_val = _safe_value(obs_fallback, outer, 'Mean')\n",
    "                row_obs[outer] = obs_val\n",
    "            for row in (row_avg, row_diff, row_obs):\n",
    "                rows.append(row)\n",
    "                panel_counts[panel_short] = panel_counts.get(panel_short, 0) + 1\n",
    "                asset_key = (panel_short, str(asset))\n",
    "                asset_counts[asset_key] = asset_counts.get(asset_key, 0) + 1\n",
    "        top_header = None\n",
    "    columns = ['Panel', 'Asset', 'Statistic', 'RowType'] + list(value_columns)\n",
    "    tidy_df = pd.DataFrame(rows)\n",
    "    if tidy_df.empty:\n",
    "        tidy_df = pd.DataFrame(columns=columns)\n",
    "    else:\n",
    "        tidy_df = tidy_df[columns]\n",
    "    return {\n",
    "        'data': tidy_df,\n",
    "        'value_columns': list(value_columns),\n",
    "        'panel_counts': panel_counts,\n",
    "        'asset_counts': asset_counts,\n",
    "        'top_header': top_header,\n",
    "        'kind': kind,\n",
    "    }\n",
    "\n",
    "def _format_cell_value(value, *, bold=False, small=False, is_count=False):\n",
    "    if pd.isna(value):\n",
    "        text = ''\n",
    "    else:\n",
    "        if is_count:\n",
    "            text = f\"{int(round(float(value))):,}\"\n",
    "        else:\n",
    "            text = f\"{float(value):,.3f}\"\n",
    "        if bold and text:\n",
    "            text = f\"\\textbf{{{text}}}\"\n",
    "    if small:\n",
    "        return f\"{{\\footnotesize {text}}}\" if text else \"{\\footnotesize }\"\n",
    "    return text\n",
    "\n",
    "def _build_latex_table(layout, caption):\n",
    "    tidy_df = layout['data']\n",
    "    value_columns = layout['value_columns']\n",
    "    panel_counts = layout['panel_counts']\n",
    "    asset_counts = layout['asset_counts']\n",
    "    top_header = layout['top_header']\n",
    "    align = 'lll' + 'r' * len(value_columns)\n",
    "    lines = [\n",
    "        r\"\\begin{table}[!htbp]\\centering\",\n",
    "        f\"\\caption{{{caption}}}\",\n",
    "        r\"\\small\",\n",
    "        f\"\\begin{{tabular}}{{{align}}}\",\n",
    "        r\"\\toprule\",\n",
    "    ]\n",
    "    if top_header:\n",
    "        lines.append(r\"\\multicolumn{3}{c}{} & \" + rf\"\\multicolumn{{{len(value_columns)}}}{{c}}{{{top_header}}} \\\")\n",
    "    header = ['Panel', 'Asset', 'Statistic'] + value_columns\n",
    "    lines.append(' & '.join(header) + \" \\\")\n",
    "    lines.append(r\"\\midrule\")\n",
    "    used_panel = set()\n",
    "    used_asset = set()\n",
    "    for _, row in tidy_df.iterrows():\n",
    "        panel = row['Panel']\n",
    "        asset = row['Asset']\n",
    "        row_type = row['RowType']\n",
    "        if panel not in used_panel:\n",
    "            panel_cell = f\"\\multirow{{{panel_counts[panel]}}}{{*}}{{{panel}}}\"\n",
    "            used_panel.add(panel)\n",
    "        else:\n",
    "            panel_cell = ''\n",
    "        asset_key = (panel, asset)\n",
    "        if asset_key not in used_asset:\n",
    "            asset_cell = f\"\\multirow{{{asset_counts[asset_key]}}}{{*}}{{{asset}}}\"\n",
    "            used_asset.add(asset_key)\n",
    "        else:\n",
    "            asset_cell = ''\n",
    "        if row_type == 'avg':\n",
    "            stat_cell = f\"\\textbf{{{row['Statistic']}}}\"\n",
    "        else:\n",
    "            stat_cell = f\"{{\\footnotesize {row['Statistic']}}}\"\n",
    "        small = row_type != 'avg'\n",
    "        is_count = row_type == 'obs'\n",
    "        cells = [panel_cell, asset_cell, stat_cell]\n",
    "        for col in value_columns:\n",
    "            cells.append(_format_cell_value(row.get(col), bold=row_type == 'avg', small=small, is_count=is_count))\n",
    "        lines.append(' & '.join(cells) + r\" \\\")\n",
    "    lines.append(r\"\\bottomrule\")\n",
    "    lines.append(r\"\\end{tabular}\")\n",
    "    lines.append('')\n",
    "    lines.append(r\"\\end{table}\")\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "def _infer_table_kind(table, basename):\n",
    "    name = basename.lower()\n",
    "    if 'daily' in name:\n",
    "        return 'daily'\n",
    "    if 'intraday' in name:\n",
    "        return 'intraday'\n",
    "    if isinstance(table.columns, pd.MultiIndex):\n",
    "        top_labels = [str(level) for level in table.columns.get_level_values(0)]\n",
    "    else:\n",
    "        top_labels = [str(col) for col in table.columns]\n",
    "    if any('Day' in label for label in top_labels):\n",
    "        return 'daily'\n",
    "    return 'intraday'\n",
    "\n",
    "def export_table(table, subdir, basename, caption):\n",
    "    out_dir = OUTPUT_ROOT / subdir\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    kind = _infer_table_kind(table, basename)\n",
    "    layout = _prepare_table_layout(table, kind)\n",
    "    tidy_df = layout['data']\n",
    "    excel_df = tidy_df.drop(columns=['RowType']) if 'RowType' in tidy_df.columns else tidy_df.copy()\n",
    "    excel_path = out_dir / f\"{basename}.xlsx\"\n",
    "    excel_df.to_excel(excel_path, index=False)\n",
    "    latex_content = _build_latex_table(layout, caption)\n",
    "    latex_path = out_dir / f\"{basename}.tex\"\n",
    "    latex_path.write_text(latex_content, encoding='utf-8')\n",
    "    print('Saved', excel_path)\n",
    "    print('Saved', latex_path)\n",
    "\n",
    "export_table(table1, 'table1', 'Table1_daily_volume_total', 'Daily volume around FOMC (Total period)')\n",
    "export_table(table2, 'table2', 'Table2_intraday_volume_total', 'Intraday volume around FOMC (Total period)')\n",
    "\n",
    "periods = ['Pre-ZLB', 'ZLB', 'Post-ZLB']\n",
    "for period in periods:\n",
    "    export_table(\n",
    "        table3_daily[period],\n",
    "        'table3',\n",
    "        f'Table3_{period}_daily_volume',\n",
    "        f'Daily volume around FOMC ({period})'\n",
    "    )\n",
    "    export_table(\n",
    "        table3_intraday[period],\n",
    "        'table3',\n",
    "        f'Table3_{period}_intraday_volume',\n",
    "        f'Intraday volume around FOMC ({period})'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Stacked Plots ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fomc_stacked_by_period(asset='', k=1, hft_sets=None, fomc_sets=None, extra_metrics=None):\n",
    "    \"\"\"Helper to run plot_fomc_k_window sequentially across policy regimes.\"\"\"\n",
    "    default_hft_sets = {\n",
    "        'Pre-ZLB': pre_zlb_hft,\n",
    "        'ZLB': zlb_hft,\n",
    "        'Post-ZLB': post_zlb_hft,\n",
    "    }\n",
    "    default_fomc_sets = {\n",
    "        'Pre-ZLB': pre_zlb_fomc,\n",
    "        'ZLB': zlb_fomc,\n",
    "        'Post-ZLB': post_zlb_fomc,\n",
    "    }\n",
    "    hft_map = hft_sets or default_hft_sets\n",
    "    fomc_map = fomc_sets or default_fomc_sets\n",
    "\n",
    "    results = {}\n",
    "    for label, hft_df in hft_map.items():\n",
    "        fomc_df = fomc_map.get(label, fomc_data)\n",
    "        res = plot_fomc_k_window(\n",
    "            hft_input=hft_df,\n",
    "            fomc_input=fomc_df,\n",
    "            k=k,\n",
    "            asset=asset,\n",
    "            period=label,\n",
    "            extra_metrics=extra_metrics,\n",
    "        )\n",
    "        results[label] = res\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Periodic Price/Volume Report ==========\n",
    "We have \n",
    "Pre-ZLB, ZLB, Post-ZLB\n",
    "Then run：\n",
    "```python\n",
    "plot_fomc_stacked_by_period('FF1', k=1)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fomc_stacked_by_period(\n",
    "    asset='Emini',\n",
    "    k=2,\n",
    "    hft_sets={\n",
    "        'Pre-ZLB': pre_zlb_emini,\n",
    "        'ZLB': zlb_emini,\n",
    "        'Post-ZLB': post_zlb_emini,\n",
    "    },\n",
    "    fomc_sets={\n",
    "        'Pre-ZLB': pre_zlb_fomc,\n",
    "        'ZLB': zlb_fomc,\n",
    "        'Post-ZLB': post_zlb_fomc,\n",
    "    },\n",
    "    extra_metrics={'Count': 'Emini_Count'},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fomc_stacked_by_period(asset='FF1', k=2)\n",
    "plot_fomc_stacked_by_period(asset='FF2', k=2)\n",
    "plot_fomc_stacked_by_period(asset='ED2', k=2)\n",
    "plot_fomc_stacked_by_period(asset='ED3', k=2)\n",
    "plot_fomc_stacked_by_period(asset='ED4', k=2)\n",
    "plot_fomc_stacked_by_period(asset='2yr', k=2)\n",
    "plot_fomc_stacked_by_period(asset='5yr', k=2)\n",
    "plot_fomc_stacked_by_period(asset='10yr', k=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}